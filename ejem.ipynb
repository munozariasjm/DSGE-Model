{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:abcpy.statisticslearning:Generation of data...\n",
      "INFO:abcpy.statisticslearning:Data generation finished.\n",
      "INFO:abcpy.statisticslearning:Learning of the transformation...\n",
      "INFO:abcpy.statisticslearning:Finished learning the transformation.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Pytorch is required to instantiate an element of the SemiautomaticNN class, in order to handle neural networks. Please install it. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-51ee552abe78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mjournal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0manalyse_journal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjournal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-51ee552abe78>\u001b[0m in \u001b[0;36minfer_parameters\u001b[0;34m(steps, n_sample, n_samples_per_param, logging_level)\u001b[0m\n\u001b[1;32m     65\u001b[0m     statistics_learning = SemiautomaticNN([height], statistics_calculator, backend,\n\u001b[1;32m     66\u001b[0m                                           \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                           n_samples_per_param=1, seed=1, early_stopping=True)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Redefine the statistics function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.7/site-packages/abcpy/statisticslearning.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, statistics_calc, backend, embedding_net, n_samples, n_samples_val, n_samples_per_param, parameters, simulations, parameters_val, simulations_val, early_stopping, epochs_early_stopping_interval, start_epoch_early_stopping, seed, cuda, scale_samples, batch_size, n_epochs, load_all_data_GPU, lr, optimizer, scheduler, start_epoch_training, optimizer_kwargs, scheduler_kwargs, loader_kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m                                               \u001b[0mstart_epoch_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                                               \u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                                               scheduler_kwargs=scheduler_kwargs, loader_kwargs=loader_kwargs)\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.7/site-packages/abcpy/statisticslearning.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, statistics_calc, backend, training_routine, distance_learning, embedding_net, n_samples, n_samples_val, n_samples_per_param, parameters, simulations, parameters_val, simulations_val, seed, cuda, scale_samples, quantile, **training_routine_kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m             raise ImportError(\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34m\"Pytorch is required to instantiate an element of the {} class, in order to handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \"neural networks. Please install it. \".format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# set random seed for torch as well:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Pytorch is required to instantiate an element of the SemiautomaticNN class, in order to handle neural networks. Please install it. "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def infer_parameters(steps=3, n_sample=250, n_samples_per_param=10, logging_level=logging.WARN):\n",
    "    \"\"\"Perform inference for this example.\n",
    "    Parameters\n",
    "    ----------\n",
    "    steps : integer, optional\n",
    "        Number of iterations in the sequential PMCABC algoritm (\"generations\"). The default value is 3\n",
    "    n_samples : integer, optional\n",
    "        Number of posterior samples to generate. The default value is 250.\n",
    "    n_samples_per_param : integer, optional\n",
    "        Number of data points in each simulated data set. The default value is 10.\n",
    "    Returns\n",
    "    -------\n",
    "    abcpy.output.Journal\n",
    "        A journal containing simulation results, metadata and optionally intermediate results.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging_level)\n",
    "    # define backend\n",
    "    # Note, the dummy backend does not parallelize the code!\n",
    "    from abcpy.backends import BackendDummy as Backend\n",
    "    backend = Backend()\n",
    "\n",
    "    # define observation for true parameters mean=170, std=15\n",
    "    height_obs = [160.82499176, 167.24266737, 185.71695756, 153.7045709, 163.40568812, 140.70658699, 169.59102084,\n",
    "                  172.81041696, 187.38782738, 179.66358934, 176.63417241, 189.16082803, 181.98288443, 170.18565017,\n",
    "                  183.78493886, 166.58387299, 161.9521899, 155.69213073, 156.17867343, 144.51580379, 170.29847515,\n",
    "                  197.96767899, 153.36646527, 162.22710198, 158.70012047, 178.53470703, 170.77697743, 164.31392633,\n",
    "                  165.88595994, 177.38083686, 146.67058471763457, 179.41946565658628, 238.02751620619537,\n",
    "                  206.22458790620766, 220.89530574344568, 221.04082532837026, 142.25301427453394, 261.37656571434275,\n",
    "                  171.63761180867033, 210.28121820385866, 237.29130237612236, 175.75558340169619, 224.54340549862235,\n",
    "                  197.42448680731226, 165.88273684581381, 166.55094082844519, 229.54308602661584, 222.99844054358519,\n",
    "                  185.30223966014586, 152.69149367593846, 206.94372818527413, 256.35498655339154, 165.43140916577741,\n",
    "                  250.19273595481803, 148.87781549665536, 223.05547559193792, 230.03418198709608, 146.13611923127021,\n",
    "                  138.24716809523139, 179.26755740864527, 141.21704876815426, 170.89587081800852, 222.96391329259626,\n",
    "                  188.27229523693822, 202.67075179617672, 211.75963110985992, 217.45423324370509]\n",
    "\n",
    "    # define prior\n",
    "    from abcpy.continuousmodels import Uniform\n",
    "    mu = Uniform([[150], [200]], name=\"mu\")\n",
    "    sigma = Uniform([[5], [25]], name=\"sigma\")\n",
    "\n",
    "    # define the model\n",
    "    from abcpy.continuousmodels import Normal\n",
    "    height = Normal([mu, sigma], )\n",
    "\n",
    "    # define statistics\n",
    "    from abcpy.statistics import Identity\n",
    "    statistics_calculator = Identity(degree=3, cross=True)\n",
    "\n",
    "    # Learn the optimal summary statistics using Semiautomatic summary selection\n",
    "    from abcpy.statisticslearning import Semiautomatic\n",
    "    statistics_learning = Semiautomatic([height], statistics_calculator, backend,\n",
    "                                        n_samples=1000, n_samples_per_param=1, seed=1)\n",
    "\n",
    "    # Redefine the statistics function\n",
    "    new_statistics_calculator = statistics_learning.get_statistics()\n",
    "\n",
    "    # Learn the optimal summary statistics using SemiautomaticNN summary selection;\n",
    "    # we use 200 samples as a validation set for early stopping:\n",
    "    from abcpy.statisticslearning import SemiautomaticNN\n",
    "    statistics_learning = SemiautomaticNN([height], statistics_calculator, backend,\n",
    "                                          n_samples=1000, n_samples_val=200,\n",
    "                                          n_samples_per_param=1, seed=1, early_stopping=True)\n",
    "\n",
    "    # Redefine the statistics function\n",
    "    new_statistics_calculator = statistics_learning.get_statistics()\n",
    "\n",
    "    # define distance\n",
    "    from abcpy.distances import Euclidean\n",
    "    distance_calculator = Euclidean(new_statistics_calculator)\n",
    "\n",
    "    # define kernel\n",
    "    from abcpy.perturbationkernel import DefaultKernel\n",
    "    kernel = DefaultKernel([mu, sigma])\n",
    "\n",
    "    # define sampling scheme\n",
    "    from abcpy.inferences import PMCABC\n",
    "    sampler = PMCABC([height], [distance_calculator], backend, kernel, seed=1)\n",
    "\n",
    "    eps_arr = np.array([500])  # starting value of epsilon; the smaller, the slower the algorithm.\n",
    "    # at each iteration, take as epsilon the epsilon_percentile of the distances obtained by simulations at previous\n",
    "    # iteration from the observation\n",
    "    epsilon_percentile = 10\n",
    "    journal = sampler.sample([height_obs], steps, eps_arr, n_sample, n_samples_per_param, epsilon_percentile)\n",
    "\n",
    "    return journal\n",
    "\n",
    "\n",
    "def analyse_journal(journal):\n",
    "    # output parameters and weights\n",
    "    print(journal.opt_values)\n",
    "    print(journal.get_weights())\n",
    "\n",
    "    # do post analysis\n",
    "    print(journal.posterior_mean())\n",
    "    print(journal.posterior_cov())\n",
    "\n",
    "    # print configuration\n",
    "    print(journal.configuration)\n",
    "\n",
    "    # plot posterior\n",
    "    journal.plot_posterior_distr(path_to_save=\"posterior.png\")\n",
    "\n",
    "    # save and load journal\n",
    "    journal.save(\"experiments.jnl\")\n",
    "\n",
    "    from abcpy.output import Journal\n",
    "    new_journal = Journal.fromFile('experiments.jnl')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    journal = infer_parameters(logging_level=logging.INFO)\n",
    "    analyse_journal(journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
